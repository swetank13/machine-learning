{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62691abb-784b-42c8-8645-f6aca9e5c3fb",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Sequence Models</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee17c24-9e5d-4042-9e58-adb5c310bf59",
   "metadata": {},
   "source": [
    "#### 11.1: Sequence Model\n",
    "- Sequential data refers to data where the order of elements matters, such as time series, text, audio, video, etc.\n",
    "- Sequential models are a broad class of models designed to process sequential data.\n",
    "- Sequence models process sequential data, making them ideal for tasks like time series, speech, and text processing.\n",
    "- Examples of sequence models include RNNs, LSTMs, GRUs, and Transformers, each designed for specific challenges.\n",
    "- Sequence models capture dependencies between elements, enabling predictions based on context.\n",
    "- Applications range from language translation and text generation to stock price prediction and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c42654-549e-46c2-90c9-ebd3e0a60256",
   "metadata": {},
   "source": [
    "#### 11.2: RNN in Depth\n",
    "- Issues with regular neural network:\n",
    "  1. Regular neural networks require a fixed size input whereas sequences vary in length.\n",
    "  2. Regular neural networks do not consider order of elements in a sequence.\n",
    "  3. No parameter sharing\n",
    "- Benefits of RNN\n",
    "  - Designed to work with sequential data. Effective for tasks where order and context matter.\n",
    "  - In-built memory mechanism\n",
    "  - Parameter sharing\n",
    "- Recurrent Neural Networks (RNNs) are specialized for sequential data, processing inputs step-by-step while maintaining a memory of past information.\n",
    "- RNNs use hidden states to capture temporal dependencies, enabling predictions based on sequence history.\n",
    "- Ideal for tasks like text generation, speech recognition, and time-series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31914a-96a6-4f83-bf48-595de4e6614f",
   "metadata": {},
   "source": [
    "#### 11.3: Types of RNN\n",
    "- One-to-Many RNNs generate sequences from a single input, like caption generation from an image.\n",
    "- Many-to-One RNNs summarize sequences into a single output, such as sentiment analysis of a sentence.\n",
    "- Many-to-Many RNNs handle sequence input and output, such as machine translation or video frame labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e96fd9-7f9e-41aa-a560-88be4b2bf44f",
   "metadata": {},
   "source": [
    "#### 11.4: Vanishing Gradient Problem\n",
    "- The vanishing gradient problem in fully connected neural networks occurs when gradients shrink during backpropagation, preventing earlier layers from learning effectively.\n",
    "- Solutions for Vanishing Gradient\n",
    "  1. ReLU Activation\n",
    "  2. Batch Normalization\n",
    "  3. Residual Connections\n",
    "- The exploding gradient problem in fully connected neural networks occurs when gradients grow uncontrollably during backpropagation, causing unstable training and large weight updates.\n",
    "- Note: RNN learns via backpropagation through time.\n",
    "- Solutions to Vanishing Gradient Problem\n",
    "  1. LSTM\n",
    "  2. GRU\n",
    "  3. Residual Connections\n",
    "- Vanishing gradients occur when gradients become too small during backpropagation, hindering effective weight updates.\n",
    "- It primarily affects deep networks with activation functions like sigmoid or tanh, leading to slow or stalled learning.\n",
    "- Layers closer to the input experience smaller gradients, causing them to learn much slower than deeper layers.\n",
    "- Solutions include using activation functions like ReLU, batch normalization, or architectures like LSTMs with gating mechanisms.\n",
    "Addressing vanishing gradients is critical for training deep neural networks effectively and efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d283d23-ffed-4709-8eb8-37c2f3b5443a",
   "metadata": {},
   "source": [
    "#### 11.5: LSTM (Long Short Term Memory Network)\n",
    "- Long Short Term Memory (LSTM) network addresses short term memory problem in RNN by introducing long term memory cell (a.k.a cell state).\n",
    "- It has both short term and long term memory.\n",
    "- It has 3 gates: Forget, Input, and Output.\n",
    "- LSTM, GRU etc. have become less popular in the shiny world of transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1edb80-5d6f-470b-ac86-f4e3cd38d1dc",
   "metadata": {},
   "source": [
    "#### 11.6: GRU\n",
    "- GRU combines long and short term memory in one cell.\n",
    "- It is lightweight, more efficient. LSTM, however, performs well on longer sequences.\n",
    "- GRU has only two gates: Update and Reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b5dc7-2b91-4606-b325-716f9dac75aa",
   "metadata": {},
   "source": [
    "#### 11.7: MCQ\n",
    "- Which of these tasks is ideal for using RNNs?\n",
    "  - Speech recognition and text generation\n",
    "- What is a key feature of RNNs (Recurrent Neural Networks)?\n",
    "  - They process data step-by-step and maintain a memory of past information\n",
    "- What is the primary purpose of Sequence Models in deep learning?\n",
    "  - To process sequential data for tasks like time series, speech, and text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
